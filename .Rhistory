data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
#fit a ridge regresstion
fitControl <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10)
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
warnings()
data.frame(lambda=c(0.001,0.5,5,30,70,400))
#' Ridge regression
#'
#' Computes a ridge model using the Ridge -square method
#' The function also fitted values.
#'
#' @param x    independent variable (x)
#'
#' @param y               The dependent (y)
#'
#' @param data       a data set from which the used variables are taken
#' @return an list of class 'ridgereg' containing beta coefficients and fitted values.
#'
#' @details
#'  These measures are stored in a list of class 'ridgereg'.
#'
#'
#' @references
#'
#'
#' @examples
#' ## Make a simple linear regression of how the Petal.Length responds to the Petal.Width in the dataset iris.
#'    ridgereg(Petal.Length ~ Species, lambda = 5, iris)
#'
#' @export
ridgeregXY<-function(y,x , lambda = 0){
#des.mat <- cbind(rep(1, nrow(x)), x)
#colnames(des.mat)[1]<-"(Intercept)"
#dep.var<-as.matrix(y)
#des.mat[,2:ncol(des.mat)] <- scale(des.mat[,2:ncol(des.mat)])
#des.mat<-as.matrix(des.mat)
#lambda<-diag(lambda, nrow=dim(t(des.mat)%*%des.mat)[1], ncol=dim(t(des.mat)%*%des.mat)[2])
#des.mat<- as.numeric(as.matrix(x))
#dep.var<- as.matrix(as.numeric(y))
#des.mat<- scale(des.mat)
#des.mat<- as.matrix(des.mat)
#lambda<-diag(lambda, nrow=dim(t(des.mat)%*%des.mat)[1], ncol=dim(t(des.mat)%*%des.mat)[2])
#BETAridge <- solve((t(des.mat)%*%des.mat) + lambda) %*% t(des.mat) %*% (dep.var)
#y.hat <- (des.mat %*% BETAridge)
#l<-list(coefficients =BETAridge, y.hat = y.hat)
#class(l) <- "ridgereg"
#l$call <- match.call()
#return(l)
X<-as.matrix(x)
y<-as.numeric(y)
I <- matrix(0,nrow=ncol(X),ncol=ncol(X))
diag(I) <- 1
ans <- solve(t(X)%*%X+lambda*I)%*%(t(X)%*%y)
beta <- as.vector(ans)
names(beta) <- colnames(X)
#the fitted values
fit <- X %*% beta
fit <- as.vector(fit)
#the residuals
res <- y - fit
fit.res<-data.frame(fit,res)
names(fit.res)<-c("fit","res")
res <- as.vector(res)
fit.res <- data.frame(fit, res)
names(fit.res) <- c("fit", "res")
#the degree of freedoms
n <- nrow(X)
p <- ncol(X)
df <- n - p
l <- list(coefficients = beta, y.hat = fit, residuals = res)
l$call <- match.call()
class(l) <- "ridgereg"
return(l)
}
#colnam
library(lab4)
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
#fit a ridge regresstion
fitControl <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10)
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
#fit a ridge regresstion
fitControl <- trainControl(
method = "repeatedcv",
number = 6,
repeats = 6)
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
warnings()
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
#fit a ridge regresstion
fitControl <- trainControl(
method = "repeatedcv",
number = 6,
repeats = 6)
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "repeatedcv",
number = 6,
repeats = 6)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
library(lab4)
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
#fit a ridge regresstion
fitControl <- trainControl(
method = "repeatedcv",
number = 6,
repeats = 6)
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "repeatedcv",
number = 6,
repeats = 6)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
#ridgeregXY$label<-"Ridge"
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "cv",
number = 10,
repeats = 10)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
test<-lm.ridge(y~.,data=longley, lambda=2)
library(MASS)
test<-lm.ridge(y~.,data=longley, lambda=2)
pred(test)
predict(test)
test$kLW
test$Inter
test$scales
test$xm
getModelInfo("ridge")
library(caret)
library(mlbench)
data(BostonHousing)
set.seed(3456)
trainIndex <- createDataPartition(BostonHousing$rad, p = .5,
list = FALSE,
times = 1)
#stratifierat urval på rad, olkart om det är bra.
bostonTrain <- BostonHousing[trainIndex,]
bostonTest <- BostonHousing[-trainIndex,]
head(trainIndex)
#train() method = 'leapForward'
plsFit <- train(crim ~ . , data = bostonTrain, method = "leapForward")
## Center and scale the predictors for the training
## setvand all future samples.
#preProc = c("center", "scale")
summary(plsFit)
plot(plsFit)
predictions<-as.data.frame(x=
predict(plsFit,newdata = bostonTest)
)#predict hittar de variabler den ska använda.
colnames(predictions) <- "pred"
ggplot()+geom_point(data = bostonTest,aes(y=crim,x=c(1:length(bostonTest$crim))))+
geom_point(data=predictions,aes(y = pred,x=c(1:length(bostonTest$crim))),col="red")
#create a new list
ridgeregXY <- list(type=c("Regression"),
library="lab4",
loop=NULL)
#The parameter elements
ridgeregXY$parameters <- data.frame(parameter="lambda",
class="numeric",
label="lambda")
#lmGrid <- function(x, y, len = NULL, search = "grid") {
# lambda <- data.frame( = seq(0,200,by=10))
#}
#ridgeregXY$grid <- lmGrid
Fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
ridgeregXY(y=y, x=x, lambda=param$lambda)$y.hat
}
ridgeregXY$fit <- Fit
ridgeregXY$prob<-list(NULL)
ridgeregXY$predict<-function(modelFit, newdata, preProc=NULL, submodels=NULL){
pred(modelFit, newdata)
}
ridgeregXY$sort<-function (x) x[order(-x$lambda), ]
ridgeregXY$label<-"ridgeregXY"
ridgeregXY$grid <- function(y,x, len=NULL, search="grid"){
data.frame(lambda=c(0.001,0.5,5,30,70,400))
}
fitControl <- trainControl(
method = "cv",
number = 10,
repeats = 10)
set.seed(3245)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
#lm.ridge
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridge, trControl = fitControl)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=lm.ridge, trControl = fitControl)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=Ridge, trControl = fitControl)
lm.ridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=Ridge, trControl = fitControl)
lm.ridge
myridge <- train(y=bostonTrain$crim, x=is.numeric(bostonTrain[,-1]), method=ridgeregXY, trControl = fitControl)
warnings()
library(lab4)
library(lab4)
library(MASS)
library(lab4)
library(lab4)
library(devtools)
